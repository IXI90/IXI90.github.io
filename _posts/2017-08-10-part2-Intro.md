---
layout: post
comments: true
title:  "Quick Introduction to Reinforcement Learning: Part 2 - Dynamic Programming"
excerpt: "The different Bellman Equations will be introduced. Afterwards we discuss Value and Policy Iteration and sketch proofs of those dynamic programming algorithms. "
date:   2017-08-10
mathjax: true
---


## Dynamic Programming

Dynamic Programming (DP) is an optimization approach that transforms complex problems into overlapping, simpler sub-problems. In our context, the term DP refers to a class of algorithms that is able to compute optimal policies in a known MDP, i.e. $<\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$ is given. In later posts the assumption of a known environment will be dropped (model-free setting) - but even in those settings the ideas behind DP are still relevant.

We concentrate on infinite-horizon discounted MDPs. Throughout the rest of this post we assume that both the state as well as the action space are finite. Besides, $\gamma < 1$ and the reward function is bounded by $M \in \mathbb{R}$. One nice consequence of these assumptions is the existence of an optimal policy.

## Bellman Equations

The definitions of the different (optimal) value functions can be rewritten in a recursive manner, which gives us the famous Bellman (Optimality) Equations: Fix policy $\pi \in \Pi$, and an arbitrary state $s \in \mathcal{S}$:

$$ V_{\pi}(s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] = \mathbb{E} _{\pi} [R_0 + \gamma \sum_{t=1} \gamma^{t-1} R_t \mid s] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s' ] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') V _{\pi} (s') $$

Analogously, 

$$ Q_{\pi} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q_{\pi} (s', \pi (s')) $$

$$ Q^{\ast} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s')  = \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') sup_{a' \in \mathcal{A}} Q^{\ast} (s', a')$$

$$ V^{\ast} (s) = sup_{a \in \mathcal{A}} Q^{\ast} (s,a) = sup_{a \in \mathcal{A}} ( \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s') ) $$

It should be noted that $V_{\pi}$ can be interpreted as an element of $\mathbb{R}^{ \mid \mathcal{S} \mid}$, since $V_{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ and $ \mid \mathcal{S} \mid < \infty$ (finite state space). Similarly, $Q_{\pi}$ can be seen as an element of $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$ (use: $\mid \mathcal{A} \mid < \infty$). At the same time it has to be clear that not every element in $\mathbb{R}^{ \mid \mathcal{S} \mid}$ (or in $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$) is related to a state-value function (or action-value function): simply realize that in our case there only exists a finite number of deterministic policies, hence only a finite number of different value functions. Any vector space over the real numbers is certainly not finite. 

Let us equip $\mathbb{R}^{ \mid \mathcal{S} \mid}$ and  $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$ with the supremum norm (maximum norm) $ | \cdot | _{\infty}$



  
    