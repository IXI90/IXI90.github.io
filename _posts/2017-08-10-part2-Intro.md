---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 2 - Dynamic Programming"
excerpt: "The different Bellman Equations will be introduced. Afterwards we discuss Value and Policy Iteration and sketch proofs of those dynamic programming algorithms. "
date:   2017-08-10
mathjax: true
---


## Dynamic Programming

Dynamic Programming (DP) is an optimization approach that transforms complex problems into overlapping, simpler sub-problems. In our context, the term DP refers to a class of algorithms that is able to compute optimal policies in a known MDP, i.e. $<\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$ is given. In later posts the assumption of a known environment will be dropped (model-free setting) - but even in those settings the ideas behind DP are still relevant.

We concentrate on infinite-horizon discounted MDPs. Throughout the rest of this post we assume that both the state as well as the action space are finite. Besides, $\gamma < 1$ and the reward function is bounded by $M \in \mathbb{R}$. One nice consequence of these assumptions is the existence of an optimal policy.

### Bellman Equations

The definitions of the different (optimal) value functions can be rewritten in a recursive manner, which gives us the famous Bellman (Optimality) Equations: Fix policy $\pi \in \Pi$, and an arbitrary state $s \in \mathcal{S}$:

$$ V_{\pi}(s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] = \mathbb{E} _{\pi} [R_0 + \gamma \sum_{t=1} \gamma^{t-1} R_t \mid s] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s' ] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') V _{\pi} (s') $$

Analogously, 

$$ Q_{\pi} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q_{\pi} (s', \pi (s')) $$

$$ Q^{\ast} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s')  = \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') sup_{a' \in \mathcal{A}} Q^{\ast} (s', a')$$

$$ V^{\ast} (s) = sup_{a \in \mathcal{A}} Q^{\ast} (s,a) = sup_{a \in \mathcal{A}} ( \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s') ) $$

It should be noted that $V_{\pi}$ can be interpreted as an element of $\mathbb{R}^{ \mid \mathcal{S} \mid}$, since $V_{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ and $ \mid \mathcal{S} \mid < \infty$ (finite state space). Similarly, $Q_{\pi}$ can be seen as an element of $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$ (use: $\mid \mathcal{A} \mid < \infty$). At the same time it has to be clear that not every element in $\mathbb{R}^{ \mid \mathcal{S} \mid}$ (or in $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$) is related to a state-value function (or action-value function): consider that in our case there only exists a finite number of deterministic policies, hence only a finite number of different value functions. Any vector space over the real numbers is certainly not finite. 

Let us equip $\mathbb{R}^{ \mid \mathcal{S} \mid}$ and  $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$ with the supremum norm (maximum norm) $ \mid \cdot \mid _{\infty}$. Due to the fact that all norms are equivalent on a finite dimensional vector space, we can still be sure that our considered spaces are complete. The Bellman Equations above can be used to define (not necessarily linear) operators on $\mathbb{R}^{ \mid \mathcal{S} \mid}$ and  $\mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid}$:

$$ T^{\pi}: \mathbb{R}^{ \mid \mathcal{S} \mid} \rightarrow \mathbb{R}^{ \mid \mathcal{S} \mid}$$, 
$$ V(s) \mapsto \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') V _{\pi} (s') $$


$$ T^{\ast} : \mathbb{R}^{ \mid \mathcal{S} \mid} \rightarrow \mathbb{R}^{ \mid \mathcal{S} \mid} $$,
$$ V(s) \mapsto sup_{a \in \mathcal{A}} ( \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s') ) $$

$$ L^{\pi}: \mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid} \rightarrow \mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid} $$,
$$ Q(s,a) \mapsto \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q (s', \pi (s')) $$

$$ L^{\ast}: \mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid} \rightarrow \mathbb{R}^{\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid} $$
$$ Q(s,a) \mapsto \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') sup_{a' \in \mathcal{A}} Q (s', a')$$

Due to the Bellman Equations we already know one fixpoint for every operator (e.g. $V_{\pi}$ is a fixpoint of $T^{\pi}$, and $V^{\ast}$ is a fixpoint of $T^{\ast}$). 

Even if the MDP is known, it is rather complicated to calculate value functions directly (main reason: size of MDP). As a consequence, one searches for iterative algorithms that converge to the desired value functions. Often this is called "evaluation". 
Let's assume we have a deterministic policy $\pi \in \Pi$ in mind and we want to calculate the corresponding state-value function $V_{\pi}$. As mentioned above, ($\mathbb{R}^{ \mid \mathcal{S} \mid}$, $ \mid \cdot \mid _{\infty}$ ) is complete and for $V, V' \in \mathbb{R}^{ \mid \mathcal{S} \mid}$

$$ \mid T^{\pi} (V) - T^{\pi} (V')  \mid_{\infty} = \mid \mathcal{R} (\cdot,\pi(\cdot)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (\cdot, \pi (\cdot), s') V (s') -  \mathcal{R} (\cdot,\pi(\cdot)) - \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (\cdot, \pi (\cdot), s') V' (s') \mid _{\infty} \\ = \mid \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (\cdot, \pi (\cdot), s') (V (s') - V' (s'))\mid _{\infty} \leq \gamma \mid V (\cdot) - V' (\cdot) \mid _{\infty}$$,

i.e. $T^{\pi}$ is a contraction mapping (note: $\gamma$ < 1). Thus the Banach fixed-point theorem can be applied. As a consequence, $T^{\pi}$ admits a unique fixed-point. Furthermore, the sequence $(V_n) _{\mathbb{N}} $ with $V _{n+1} := T^ {\pi} (V_n)$ and with an arbitrary element $V_1$ converges to the unique fixed-point. However, we already know that $V _{\pi}$ has to be this fixpoint! 
So, the Banach fixed-point theorem gives us an iterative algorithm that converges to the state-value function. 

Analogously, one can apply the Banach fixed-point theorem to the other operators $T^{\ast}$, $L^{\pi}$ and $L^{\ast}$ and in this way we obtain iterative algorithms for the calculation of $V^{\ast}$, $Q_{\pi}$ and $Q^{\ast}$.

### Value Iteration

 



  
    
