---
layout: post
comments: true
title:  "Quick Introduction to Reinforcement Learning: Part 2 - Dynamic Programming"
excerpt: "The different Bellman Equations will be introduced. Afterwards we discuss Value and Policy Iteration and sketch proofs of those dynamic programming algorithms. "
date:   2017-08-10
mathjax: true
---


## Dynamic Programming

Dynamic Programming (DP) is an optimization approach that transforms complex problems into overlapping, simpler sub-problems. In our context, the term DP refers to a class of algorithms that is able to compute optimal policies in a known MDP, i.e. $<\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$ is given. In later posts the assumption of a known environment will be dropped (model-free setting) - but, even then the ideas behind DP are still relevant.

We concentrate on infinite-horizon discounted MDPs. Throughout the rest of this post we assume that both the sate as well as the action space are finite. Besides, $\gamma < 1$ and the reward function is bounded by $M \in \mathbb{R}$. One nice consequence of these assumptions is the existence of an optimal policy.

## Bellman Equations

The definitions of the different (optimal) value functions can be rewritten in a recursive manner, which gives us the famous Bellman (Optimality) Equations: Fix policy $\pi \in \Pi$, and an arbitrary state $s \in \mathcal{S}$

$$ V_{\pi}(s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] = \mathbb{E} _{\pi} [R_0 + \gamma \sum_{t=1} \gamma^{t-1} R_t \mid s] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s' ] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') V _{\pi} (s') $$
    