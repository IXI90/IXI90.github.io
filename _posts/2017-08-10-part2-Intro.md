---
layout: post
comments: true
title:  "Quick Introduction to Reinforcement Learning: Part 2 - Dynamic Programming"
excerpt: "The different Bellman Equations will be introduced. Afterwards we discuss Value and Policy Iteration and sketch proofs of those dynamic programming algorithms. "
date:   2017-08-10
mathjax: true
---


## Dynamic Programming

Dynamic Programming (DP) is an optimization approach that transforms complex problems into overlapping, simpler sub-problems. In our context, the term DP refers to a class of algorithms that is able to compute optimal policies in a known MDP, i.e. $<\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$ is given. In later posts the assumption of a known environment is going to be dropped (model-free setting) - but, even then the ideas behind DP are still relevant.

We concentrate on infinite-horizon discounted MDPs. Throughout the rest of this post we assume that both the sate as well as the action space are finite. Besides, $\gamma < 1$ and the reward function is bounded by $M \in \mathbb{R}$. One nice consequence of these assumptions is the existence of an optimal policy.

## Bellman Equations

    