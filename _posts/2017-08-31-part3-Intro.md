---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 3 - Q-learning"
excerpt: "The famous Q-learning algorithm will be introduced and applied to an easy environment of the OpenAI Gym. Besides, Deep Q-Networks will be presented as a powerful application of the Q-learning algorithm."
date:   2017-08-31
mathjax: true
---

## Q-learning

Q-learning is an off-policy, model-free RL algorithm and its goal is to approximate the optimal action-value function $Q ^{\ast}$, which in turn gives us an optimal policy (by acting greedily wrt $Q ^{\ast}$). In other words, this algorithm provides agents with the capability of learning to act optimally by collecting experiences in a possibly unknown MDP, without requiring them to build a concrete model of their environment. 
The Q-learning algorithm is basically an intelligent adaptation of policy iteration, which was described in our last blog post. The core idea of policy iteration was to iteratively switch between policy improvement (greedy actions) and policy evaluation (calculate related Q-function). As mentioned before policy evaluation is not a trivial procedure, since - in theory - one would need to apply the Bellman operator infinitely often for every state-action pair. In "large" MDPs this strategy is basically infeasible. 

Thus one needs to find ways to transfer those concepts into more applicable algorithms. 
In the case of Q-learning there is a difference between the evaluated policy and the behavior policy (off-policy method). This means that in every iteration step $k$ we still try to evaluate the greedy policy $\pi_k$ wrt the preceding Q-function $Q_{k-1}$ (target policy), but at the same time we will interact with the environment in terms of a slightly different policy $\bar{\pi}_k$. Furthermore, at every iteration step we wont fully evaluate the target policy $\pi_k$, but rather just apply the suitable Bellman optimality operator once for one state-action pair (asynchronous Dynammic Programming). Which state-action pair gets evaluated is fully determined by the experiences generated by the behavior policy $\bar{\pi}_k$.
As a consequence, we can not expect that $Q_k = Q _{\pi_k}$, i.e. $Q _k$ is only an estimate of $Q _{\pi_k}$. 
Due to the fact that we don't want to consider every state-action pair at every iteration step, the sequence of behavior policies $(\bar{\pi}_k) _ {k \in \mathbb{N}}$ has to guarantee that we at least visit every state-action pair every now and then. Otherwise some state-action pairs are ignored and it would be unrealistic to expect convergence of the algorithm. 

In practice, the behavior policy $\bar{\pi} _k $ is often just the $ \epsilon $ -greedy policy wrt $Q _{k-1}$ with $\epsilon \in \[ 0,1 \], \epsilon << 1$, i.e. in state $s \in \mathcal{S}$ with probability $1 - \epsilon$ take a greedy action $a \in argmax _{a \in \mathcal{A}} Q _{k-1} (s,a)$ and with probability $\epsilon$ take a random action $a \in \mathcal{A}$. Considering again an infinite-horizon MDP with a finite state- and action-space, this $\epsilon$-greedy policy $\bar{\pi}_k$ obviously makes sure that every state-action pair is visited infinitely often. 

To clarify the sketched Q-learning algorithm, let us write down a detailed version of the algorithm. In the following form of the algorithm we consider MDPs with terminal states. One sequence of experiences $<s_0,a_0,R_0,s_1,a_1,R_1,...,s_T>$, where $s_t \in \mathcal{S}, a_t \in \mathcal{A}, R_t = \mathcal{R} (s_t,a_t)$, $s_0$ an initial and $s_T$ a terminal state, is called an episode:

<img src="https://raw.githubusercontent.com/IXI90/IXI90.github.io/master/Q-learning.jpg" width="700" height="350" />

A few little aspects of the algorithm still have to be discussed. Hopefully you remember that the Bellman Optimality Operator applied to $Q_{k-1}$ at the tupel $<s_t,a_t>$ was originally defined by $ Q_{k-1} (s_t,a_t) \mapsto \mathcal{R} (s_t,a_t) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s_t,a_t,s') sup_{a' \in \mathcal{A}} Q_{k-1} (s', a')$
    
 

 


