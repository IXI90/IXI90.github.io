---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 3 - Q-learning"
excerpt: "The famous Q-learning algorithm will be introduced and applied to an easy environment of the OpenAI Gym. Besides, Deep Q-Networks will be presented as a powerful application of the Q-learning algorithm."
date:   2017-08-31
mathjax: true
---

## Q-learning

- goal approximate Qstar with the idea of policy iteration described before
- however, at each policy evaluation step we do not want to evaluate wrt every state-action pair
- extreme form, only want to update the state action pair that wie experienced just now
- consequently, still have to make sure that we visit every pair every now and then
- otherwise not reasonable to expect convergence - this forces us to not only act greedily, but epsilon greedy
- this makes Q-learning an asynchronous dynamic programming algorithm
- can be used in a setting where we do not understand the full dynamics of the MDP
- i.e. off-policy model-free algorithm

- explicit algorithm: one evaluation step by applying Bellman operator
- act epsilon greedy


- Convergence topic in Watson