---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 3 - Q-learning"
excerpt: "The famous Q-learning algorithm will be introduced and applied to an easy environment of the OpenAI Gym. Besides, Deep Q-Networks will be presented as a powerful application of the Q-learning algorithm."
date:   2017-08-31
mathjax: true
---

## Q-learning

The goal of Q-learning is to approximate the optimal action-value function $Q ^{\ast}$, which in turn gives us an optimal policy (act greedily wrt $Q ^{\ast}$).
The Q-learning algorithm is basically an intelligent adaptation of policy iteration, which was described in our last blog post. The core idea of policy iteration was to iteratively switch between policy improvement (greedy actions) and policy evaluation (calculate related Q-function). As mentioned before policy evaluation is not a trivial procedure, since in theory one would need to apply the Bellman operator infinitely often and consider every state-action pair. In "large" (possibly unknown) MDPs this is basically infeasible. 

Thus one needs to find ways to transfer those concepts into more applicable algorithms. In Q-learning there will be a difference between the evaluated policy and the behavior policy (off-policy method). This means that we will still try to evaluate the greedy policy wrt the preceding Q-function, but at the same time we will interact with the environment in terms of a slightly different policy. Furthermore, at every iteration step we wont fully evaluate the target policy, but rather just apply the suitable Bellman operator once for one (or a few) state-action pairs (asynchronous Dynammic Programming).  


- goal approximate Qstar with the idea of policy iteration described before
- i.e. it provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains
- however, at each policy evaluation step we do not want to evaluate wrt every state-action pair
- extreme form, only want to update the state action pair that wie experienced just now
- consequently, still have to make sure that we visit every pair every now and then
- otherwise not reasonable to expect convergence - this forces us to not only act greedily, but epsilon greedy
- this makes Q-learning an asynchronous dynamic programming algorithm
- can be used in a setting where we do not understand the full dynamics of the MDP
- i.e. off-policy model-free algorithm

- explicit algorithm: one evaluation step by applying Bellman operator
- act epsilon greedy
- Q_n as iterations steps


- Watson first presented Q-learning without a detailed proof, but did it in 1992 then with proof in the following setting
- tabular, finite MDP with... (finite meaning finite state, action space)
- Convergence topic in Watson (Q look up table representation)
- Theorem of Watson (conditions on alpha, bounded rewardsgo)
- convergence with probability 1 (proof with a nicely designed action-replac process)


- Example: a look-up table representation, would love to use the Roulette openAI gym setting Roulette-v0 