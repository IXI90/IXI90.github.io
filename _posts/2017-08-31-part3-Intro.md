---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 3 - Q-learning"
excerpt: "The famous Q-learning algorithm will be introduced and applied to an easy environment of the OpenAI Gym. Besides, Deep Q-Networks will be presented as a powerful application of the Q-learning algorithm."
date:   2017-08-31
mathjax: true
---

## Q-learning

Q-learning is an off-policy, model-free RL algorithm and its goal is to approximate the optimal action-value function $Q ^{\ast}$, which in turn gives us an optimal policy (act greedily wrt $Q ^{\ast}$). In other words, this algorithm provides agents with the capability of learning to act optimally by collecting experiences in a possibly unknown MDP, without requring them to build a concrete model of their environment. 
The Q-learning algorithm is basically an intelligent adaptation of policy iteration, which was described in our last blog post. The core idea of policy iteration was to iteratively switch between policy improvement (greedy actions) and policy evaluation (calculate related Q-function). As mentioned before policy evaluation is not a trivial procedure, since - in theory - one would need to apply the Bellman operator infinitely often and consider every state-action pair. In "large" MDPs this is basically infeasible. 

Thus one needs to find ways to transfer those concepts into more applicable algorithms. 
In the case of Q-learning there will be a difference between the evaluated policy and the behavior policy (off-policy method). This means that in every iteration step $k$ we will still try to evaluate the greedy policy $\pi_k$ wrt the preceding Q-function $Q_{k-1}$ (target policy), but at the same time we will interact with the environment in terms of a slightly different policy $\bar{\pi}_k$. Furthermore, at every iteration step we wont fully evaluate the target policy, but rather just apply the suitable Bellman operator once for one state-action pair (asynchronous Dynammic Programming). Which state-action pair gets evaluated is fully determined by the experiences generated by the behavior policy $\bar{\pi}_k$.
Due to the fact that we do not want to consider every state-action pair at every iteration step, the sequence of behavior policies $(\bar{\pi}_k) _ {\k \in \mathbb{N}}$ has to guarantee that we at least visit every state-action pair every now and then. In practice,    



- explicit algorithm: one evaluation step by applying Bellman operator
- act epsilon greedy
- Q_n as iterations steps


- Watson first presented Q-learning without a detailed proof, but did it in 1992 then with proof in the following setting
- tabular, finite MDP with... (finite meaning finite state, action space)
- Convergence topic in Watson (Q look up table representation)
- Theorem of Watson (conditions on alpha, bounded rewardsgo)
- convergence with probability 1 (proof with a nicely designed action-replac process)


- Example: a look-up table representation, would love to use the Roulette openAI gym setting Roulette-v0 