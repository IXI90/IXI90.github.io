---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 3 - Q-learning"
excerpt: "The famous Q-learning algorithm will be introduced and applied to an easy environment of the OpenAI Gym. Besides, Deep Q-Networks will be presented as a powerful application of the Q-learning algorithm."
date:   2017-08-31
mathjax: true
---

## Q-learning

The Q-learning algorithm is basically an intelligent adaptation of policy iteration, which was described in our last blog post. However, this time we will  


- goal approximate Qstar with the idea of policy iteration described before
- i.e. it provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains
- however, at each policy evaluation step we do not want to evaluate wrt every state-action pair
- extreme form, only want to update the state action pair that wie experienced just now
- consequently, still have to make sure that we visit every pair every now and then
- otherwise not reasonable to expect convergence - this forces us to not only act greedily, but epsilon greedy
- this makes Q-learning an asynchronous dynamic programming algorithm
- can be used in a setting where we do not understand the full dynamics of the MDP
- i.e. off-policy model-free algorithm

- explicit algorithm: one evaluation step by applying Bellman operator
- act epsilon greedy
- Q_n as iterations steps


- Watson first presented Q-learning without a detailed proof, but did it in 1992 then with proof in the following setting
- tabular, finite MDP with... (finite meaning finite state, action space)
- Convergence topic in Watson (Q look up table representation)
- Theorem of Watson (conditions on alpha, bounded rewardsgo)
- convergence with probability 1 (proof with a nicely designed action-replac process)


- Example: a look-up table representation, would love to use the Roulette openAI gym setting Roulette-v0 