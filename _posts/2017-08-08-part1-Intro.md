---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 1 - the Framework"
excerpt: "We will discuss the mathematical framework for RL problems - namely, Markov Decision Processes. The goal of this series of posts is to introduce all necessary concepts in order to understand Deep Q-Networks."
date:   2017-08-08
mathjax: true
---

## Reinforcement Learning 

This exciting part of Machine Learning deals with agents that have to take 
actions in a possibly unknown environment. Those agents try to maximize their personal utility (reward) by following a learned strategy (policy). In this series of posts, I will try to summarize my first experiences with RL. The overall goal of this series is to explain all concepts needed to understand the paper "Playing Atari with Deep Reinforcement Learning" (by V. Mnih et al.). Besides, the presented theoretical models will be used to create an agent that can play my first - still favourite - mobile phone game "Snake". I hope you enjoy the posts!      

### Markov Decision Processes

The majority of RL problems can be formalized using Markov Decision Processes (MDP). A MDP is a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$: 

*State Space* $\mathcal{S}$: The state space is the set that contains all possible environmental states. Every state $s \in \mathcal{S}$ fully characterizes the given situation, hence it contains all relevant information from the history. In other words, we consider processes that have the Markov Property, i.e. "the future is independent of the past given the present". To illustrate this, let us quickly apply this to our Snake example: a state of the game is defined by the coordinates of the body of the snake, the coordinate point of the food and the direction of our current movement.  

*Action Space* $\mathcal{A}$: The action space is the set of possible actions, which can be taken in every state of the state space. With those actions the agent tries to control the system state, thus a chosen action (usually) affects the next state of the given RL problem. For example, in every state of the game our snake has the option to go left, right, up or down. In this case, our decision in one state fully determines the following state of the game.

*Transition Function* $\mathcal{P}$: Let us assume that at time $t$ we experience the state $s_t \in \mathcal{S}$ and decide to take action $a_t \in \mathcal{A}$. Now, the system makes a transition from $s_t$ to a new state $s_{t+1} \in \mathcal{S}$ - this transition is determined by the transition function $\mathcal{P}: \mathcal{S} x \mathcal{A} x \mathcal{S} \rightarrow \[0,1 \]$. This function assigns to every tuple $<s_t, a_t, s_{t+1}>$  $\in \mathcal{S} x \mathcal{A} x \mathcal{S}$ the probability of ending up in state $s_{t+1}$ after taking action $a_t$ in state $s_t$. In other words, $\mathcal{P} (s_t,a_t,s_{t+1}) = \mathbb{P}\[s_{t+1} \mid s_t, a_t \]$. 
In the case of "Snake" this transition function is quite simple, since we are in a very deterministic setting. Here every state-action pair fully determines the following state, i.e. for every $<s,a>$ $\in \mathcal{S} x \mathcal{A}$  $\exists!$  $s' \in \mathcal{S}$ s.t. $\mathcal{P}(s,a,s')=1$.   

*Reward Function* $\mathcal{R}$: The reward function $\mathcal{R}: \mathcal{S} x \mathcal{A} \rightarrow \mathbb{R}$ tells us the expected immediate reward for some given state-action pair $<s_t, a_t>$, i.e. $\mathcal{R}(s,a)= \mathbb{E}  \[ R_t \mid s_t, a_t \]$ with experienced immediate reward $R_t$. The concept of reward and punishment, represented by the reward function, is crucial in the world of RL. We implicitly always assume that gathering as much reward as possible is the core driver behind any rational decision. If we try to formalize our snake game, we could define the reward function as the change of the game score. This would imply that the snake receives a positive reward whenever it finds food. Additionally, we could think about punishing the snake whenever it bites its own body, i.e. punish the snake whenever it terminates the game.

*Discount Factor* $\gamma$: The discount factor $\gamma \in \[0,1 \]$ indicates how much the agent values immediate reward above delayed reward. To see this we need to define the return $R :=  \sum_{t=0}^\infty \gamma^t R_t$, i.e. the return is the discounted sum of immediate rewards collected throughout the life of the agent (note: if the agent's life terminates at time $T$, simply set $R_{t}=0$ for $\forall t > T$). Maximizing the expected return can be seen as the key goal of RL. First of all, the discount factor helps us to ensure that the infinite sum does not tend to infinity. Besides, it becomes clear, that if $\gamma$ is close to $0$ the agent basically only cares about the immediate reward of the next time step ("myopic behavior"). On the other hand, if $\gamma$ is close to $1$, the agent is very far-sighted and values immediate reward as much as future reward. In a financial setting we could interpret a discount factor $\gamma < 1$ as the existence of opportunity costs. However, in games we often set $\gamma = 1$, since games usually terminate after a finite number of time steps and we often don't care about "when" rewards appear - as long as the total score is high in the end. 