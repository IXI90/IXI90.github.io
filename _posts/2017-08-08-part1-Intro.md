---
layout: post
comments: true
title:  "Quick Introduction to Reinforcement Learning: Part 1 - the Framework"
excerpt: "We will discuss the mathematical framework for RL problems - namely, Markov Decision Processes. The concept of value functions and action-value functions will be introduced. This series of posts is basically a brief summary of a workshop I held at the startup neurocat. "
date:   2017-08-08
mathjax: true
---

## Reinforcement Learning 

This exciting part of Machine Learning deals with agents that have to take 
actions in a possibly unknown environment. Those agents try to maximize their personal utility (reward) by following a learned strategy (policy). This series of posts briefly summarizes my first experiences with RL. The overall goal of this series is to sketch all concepts needed to understand the paper "Playing Atari with Deep Reinforcement Learning" (by V. Mnih et al.). In the end, the presented theoretical models will be used to create an agent that can play my first and still favourite mobile phone game "Snake". I hope you enjoy the posts!      

### Markov Decision Processes

The majority of RL problems can be formalized using Markov Decision Processes (MDP). A MDP is a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$: 

*State Space* $\mathcal{S}$: The state space is the set that contains all possible environmental states. Every state $s \in \mathcal{S}$ fully characterizes the given situation, hence it contains all relevant information from the history. In other words, we consider processes that have the Markov Property, i.e. "the future is independent of the past given the present". To illustrate this, let us quickly apply this to our Snake example: a state of the game is defined by the coordinates of the body of the snake, the coordinate point of the food and the direction of our current movement.  

*Action Space* $\mathcal{A}$: The action space is the set of possible actions, which can be taken in every state of the state space. With those actions the agent tries to control the system state, thus a chosen action (usually) affects the next state of the given RL problem. For example, in every state of the game our snake has the option to go left, right, up or down. In this case, our decision in one state fully determines the following state of the game.

*Transition Function* $\mathcal{P}$: Let us assume that at time $t$ we experience the state $s_t \in \mathcal{S}$ and decide to take action $a_t \in \mathcal{A}$ (note: we will always assume discrete time steps in this workshop). Now, the system makes a transition from $s_t$ to a new state $s_{t+1} \in \mathcal{S}$ - this transition is determined by the transition function $\mathcal{P}: \mathcal{S} x \mathcal{A} x \mathcal{S} \rightarrow \[0,1 \]$. This function assigns to every tuple $<s_t, a_t, s_{t+1}>$  $\in \mathcal{S} x \mathcal{A} x \mathcal{S}$ the probability of ending up in state $s_{t+1}$ after taking action $a_t$ in state $s_t$. In other words, $\mathcal{P} (s_t,a_t,s_{t+1}) = \mathbb{P}\[s_{t+1} \mid s_t, a_t \]$. 
In the case of "Snake" this transition function is quite simple, since we are in a very deterministic setting. Here every state-action pair fully determines the following state, i.e. for every $<s,a>$ $\in \mathcal{S} x \mathcal{A}$  $\exists!$  $s' \in \mathcal{S}$ s.t. $\mathcal{P}(s,a,s')=1$.   

*Reward Function* $\mathcal{R}$: The reward function $\mathcal{R}: \mathcal{S} x \mathcal{A} \rightarrow \mathbb{R}$ tells us the expected immediate reward for some given state-action pair $<s_t, a_t>$, i.e. $\mathcal{R}(s,a)= \mathbb{E}  \[ R_t \mid s_t, a_t \]$ with experienced immediate reward $R_t$. The concept of reward and punishment, represented by the reward function, is crucial in the world of RL. We implicitly always assume that gathering as much reward as possible is the core driver behind any rational decision. If we try to formalize our snake game, we could define the reward function as the change of the game score. This would imply that the snake receives a positive reward whenever it finds food. Additionally, we could think about punishing the snake whenever it bites its own body, i.e. punish the snake whenever it terminates the game.

*Discount Factor* $\gamma$: The discount factor $\gamma \in \[0,1 \]$ indicates how much the agent values immediate reward above delayed reward. To see this we need to define the return $R :=  \sum_{t=0}^\infty \gamma^t R_t$, i.e. the return is the discounted sum of immediate rewards collected throughout the life of the agent (note: if the agent's life terminates at time $T$, simply set $R_{t}=0$ for $\forall t > T$). Maximizing the expected return can be seen as the key goal of RL. First of all, the discount factor helps us to ensure that the infinite sum does not tend to infinity. Besides, it becomes clear, that if $\gamma$ is close to $0$ the agent basically only cares about the immediate reward of the next time step ("myopic behavior"). On the other hand, if $\gamma$ is close to $1$, the agent is very far-sighted and values immediate reward as much as future reward. In a financial setting we could interpret a discount factor $\gamma < 1$ as the existence of opportunity costs. However, in games we often set $\gamma = 1$, since games usually terminate after a finite number of time steps and we often don't care about "when" rewards appear - as long as the total score is high in the end. 

### Policies and Value Functions

Having defined the right framework for our RL problem, we can now think about how to solve the MDP. As mentioned before, the goal will be to find a strategy that can maximize the expected return in every possible state of the state space. In the RL language a "strategy" of an agent is called a policy. To be more precise: A (deterministic) policy $\pi$ is a function that outputs for each state $s \in \mathcal{S}$ an action $a \in \mathcal{A}$, i.e. $\pi: \mathcal{S} \rightarrow \mathcal{A}$. Thus a policy fully defines the behaviour of an agent. Let $\Pi$ denote the set of all possible (deterministic) policies (note: we focus on deterministic policies, but in a lot of settings it might also make sense to consider stochastic policies $\pi: \mathcal{S} x \mathcal{A} \rightarrow \[0,1\]$, e.g. if our agent is a robot and the joints can not be controlled "perfectly"). To be able to compare policies we can now define value functions. The (state-) value function $V_{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ of the policy $\pi \in \Pi$ is defined by $V_{\pi} (s) := \mathbb{E} _{\pi} \[ \sum_{t=0}^\infty \gamma^t R_{t} \mid s \]$ with $\s \in \mathcal{S}$.

Thus $V_{\pi} (s)$ is the expected value (return) we achieve, when we are in state $s$ and are going to follow policy $\pi$. 

     