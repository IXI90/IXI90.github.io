---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 1 - the Framework"
excerpt: "We will discuss the mathematical framwork for RL problems - namely, Markov Decision Processes. The goal of this series of posts is to introduce all necessary concepts in order to understand Deep Q-Networks."
date:   2017-08-08
mathjax: true
---

## Reinforcement Learning 

This exciting part of Machine Learning deals with agents that have to take 
actions in a possibly unknown environment. Those agents try to maximize their personal utility (reward) by following a learned strategy (policy). In this series of posts, I will try to summarize my first experiences with RL. The overall goal of this series is to explain all concepts needed to understand the paper "Playing Atari with Deep Reinforcement Learning" (by V. Mnih et al.). Besides, the presented theoretical models will be used to create an agent that can play my first - still favourite - mobile phone game "Snake". I hope you enjoy the posts!      

### Markov Decision Processes

The majority of RL problems can be formalized using Markov Decision Processes (MDP). A MDP is a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, Î³>$: 

*State space* $\mathcal{S}$: The state space is the set that contains all possible environmental states. Every state $s \in \mathcal{S}$ fully characterizes the given situation, hence it contains all relevant information from the history. In other words, we consider processes that have the Markov Property, i.e. "the future is independent of the past given the present". To illustrate this, let us quickly apply this to our Snake example: a state of the game is defined by the coordinates of the body of the snake, the coordinate point of the food and the direction of our current movement.  

*Action space* $\mathcal{A}$: The action space is the set of possible actions, which can be taken in every state of the state space. With those actions the agent tries to control the system state, thus a chosen action (usually) affects the next state of the given RL problem. For example, in every state of the game our snake has the option to go left, right, up or down. In this case, our decision in one state fully determines the following state of the game.

*Transition Function* $\mathcal{P}$: Let us assume that at time $t$ we experience the state $s_t \in \mathcal{S}$ and decide to take action $a_t \in \mathcal{A}$. Now, the system makes a transition from $s_t$ to a new state $s_{t+1} \in \mathcal{S}$ - this transition is determined by the transition function $\mathcal{P}: \mathcal{S} x \mathcal{A} x \mathcal{S} \rightarrow [0,1]$. This function assigns to every tuple $<s_t, a_t, s_{t+1}>$  $\in \mathcal{S} x \mathcal{A} x \mathcal{S}$ the probability of ending up in state $s_{t+1}$ after taking action $a_t$ in state $s_t$. In other words, $\mathcal{P} (s_t,a_t,s_{t+1}) = \mathbb{P}$. In the case of "Snake" this transition function is quite simple, since we are in a very deterministic setting. Here every state-action pair fully determines the following state, i.e. for every $<s,a>$ in $\in \mathcal{S} x \mathcal{A}$  $\exists! - s' \in \mathcal{S}$ s.t. $\mathcal{P}(s,a,s')=1$.   

*Reward Function* $\mathcal{R}$: The reward function $\mathcal{R}: \mathcal{S} x \mathcal{A} \rightarrow \mathbb{R}$ tells us the expected immediate reward for some given state-action pair. 