---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 1 - the Framework"
excerpt: "We will discuss the mathematical framwork for RL problems - namely, Markov Decision Processes. The goal of this series of posts is to introduce all necessary concepts in order to understand Deep Q-Networks."
date:   2017-08-08
mathjax: true
---

## Reinforcement Learning 

This exciting part of Machine Learning deals with agents that have to take actions in a possibly unknown environment. Those agents try to maximize their personal utility (reward) by following a learned strategy (policy). In this series of posts, I will try to summarize my first experiences with RL. The overall goal of this series is to explain all concepts needed to understand the paper "Playing Atari with Deep Reinforcement Learning" (by V. Mnih et al.). Besides, the presented theoretical models will be used to create an agent that can play my first - still favourite - mobile phone game "Snake". I hope you enjoy the posts!      

### Markov Decision Processes

The majority of RL problems can be formalized using Markov Decision Processes (MDP). A MDP is a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, Î³>$: 

*State space* $\mathcal{S}$: The state space is the set that contains all possible environmental states. Every state $s \in \mathcal{S}$ fully characterizes the given situation, hence it contains all relevant information from the history. In other words, we consider processes that have the Markov Property, i.e. "the future is independent of the past given the present". To illustrate this, let us quickly apply this to our Snake example: a state of the game is defined by the coordinates of the body of the snake, the coordinate point of the food and the direction of our current movement.  

*Action space* $\mathcal{A}$: The action space is the set of possible actions in every given state. 
